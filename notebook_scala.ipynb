{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependent libraries and jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.0.1`\n",
    "import $ivy.`org.apache.spark:spark-avro_2.12:3.0.1`\n",
    "interp.load.cp(os.Path(\"/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://10.0.0.36:4041\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.NotebookSparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msparkSession\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2ffafce\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msparkContext\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.NotebookSparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sparkSession = NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"My App\")\n",
    "    .getOrCreate()\n",
    "\n",
    "implicit def sparkContext: SparkContext = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load spark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-52503f08-75da-4fbb-943a-9749eacae0b4', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd2.sc:25</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"PassengerId\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Survived\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Pclass\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Sex\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Age\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SibSp\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Parch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Ticket\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Fare\"\u001b[39m, DoubleType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Cabin\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Embarked\"\u001b[39m, StringType, true, {})\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [PassengerId: string, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import sparkSession.implicits._\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"PassengerId\", StringType, true)\n",
    "    .add(\"Survived\", IntegerType, true)\n",
    "    .add(\"Pclass\", IntegerType, true)\n",
    "    .add(\"Name\", StringType, true)\n",
    "    .add(\"Sex\", StringType, true)\n",
    "    .add(\"Age\", IntegerType, true)\n",
    "    .add(\"SibSp\", IntegerType, true)\n",
    "    .add(\"Parch\", IntegerType, true)\n",
    "    .add(\"Ticket\", StringType, true)\n",
    "    .add(\"Fare\", DoubleType, true)\n",
    "    .add(\"Cabin\", StringType, true)\n",
    "    .add(\"Embarked\", StringType, true)\n",
    "\n",
    "val df = sparkSession.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .schema(schema)\n",
    "    .load(\"titanic.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [PassengerId: string, Survived: int ... 10 more fields]\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [PassengerId: string, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, testData) = df.randomSplit(Array(0.7, 0.3), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.repartition(1)\n",
    "    .write.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"train_dataset\")\n",
    "\n",
    "testData.repartition(1)\n",
    "    .write.format(\"csv\")\n",
    "    .option(\"header\", true)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"test_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
